Problem Statement:
A sensor emits a reading at a regular interval and that goes to a Kafka topic.  The reading 
The key is the Sensor ID, created time is the event time and reading is the value.  This is a patient monitoring system where the sensor is attached to a patient measuring the value.
The first two columns are start and end time, third column is the max reading value, received in the most recent 15 minutes
The solution is to get the maximum temperature in the 15-minute window 
If we say, the max temp is 38.5 at 9 am, which means we received some temperature reading from 8:45 to 9:00 and the max observation was 38.5
However, we are asked to report the outcome every five minutes
The sensor reading looks as below
SEN001:{"CreatedTime": "2021-01-05 08:45:00","Reading": 36.2}
SEN001:{"CreatedTime": "2021-01-05 08:50:00","Reading": 38.5}
SEN001:{"CreatedTime": "2021-01-05 09:03:00","Reading": 39.6}
SEN001:{"CreatedTime": "2021-01-05 09:13:00","Reading": 35.5}

Solution:
If we consider, Tumbling window of 15 minutes and trigger time of 5 minutes
The windows will not overlap. But in our case, it overlaps. Each window overlaps by 10 minutes. So, we need to use Sliding Window.
Sliding windows are an excellent tool for computing moving aggregates
Each event is most likely to participate in more than one window due to the overlaps.
Where as in Tumbling window, each event will be part of only one window.

1. Define a spark session
val spark = SparkSession.builder()
      .master("local[1]")
      .appName("Sliding Window Demo")
      .config("spark.streaming.stopGracefullyOnShutdown", "true")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()


2. Define the schema for the input sensor records

val sensorSchema = StructType(List(
      StructField("CreatedTime", StringType),
      StructField("Reading", DoubleType)
    ))

3. Read the streaming from Kafka Topics
val kafkaSrcDF = spark.
		    readStream.
                    format("kafka").
                    option("kafka.bootstrap.servers", "localhost:9092").
                    option("subscribe", "sensor").
                    option("startingOffsets", "earliest").
                    load()

4. Extract the key & value data from the kafkaSrcDF and provided alias names 
val valueDF = kafkaSrcDF.
                        select(col("key").cast("string").alias("SensorID"),from_json(
			col("value").cast("string"), invoiceSchema).alias("value"))

5. Add a new column with time formatted as below
val sensorDF = valueDF.select("SensorID", "value.*").
		       withColumn("CreatedTime", to_timestamp(col("CreatedTime"), "yyyy-MM-dd HH:mm:ss"))


6. WaterMark time set as 30 minutes, any window outside of the Watermark will be cleaned
Spark will ignore all late coming records for those windows

Grouping by SensorID ,Time Window and first argument is event time column, second window duration is set to 15 minute time and slide duration is set to 5 minutes

And compute the aggregates 

val aggDF = sensorDF.withWatermark("CreatedTime", "30 minute").
		groupBy(col("SensorID"), window(col("CreatedTime"), "15 minute", "5 minute")).
		agg(max("Reading").alias("MaxReading"))

7. Formatting the output Records 
val outputDF = aggDF.select("SensorID", "window.start", "window.end", "MaxReading")

8. Writing the output stream on console 
val windowQuery = outputDF.
		  writeStream.
		  format("console").
                  outputMode("update").
                  option("checkpointLocation", "chk-point-dir").
                  trigger(Trigger.ProcessingTime("1 minute")).start()

windowQuery.awaitTermination()

Steps to run the Application
1. Run the Zookeeper and Kafka Server
2. Create a topic
   kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sensor

3. Start the Producer
   kafka-console-producer.sh --broker-list localhost:9092 --topic sensor --property "parse.key=true" --property "key.separator=:"

4. Pass all the inputs one by one or all at once in the console producer 

The output looks 
 ----------------------------------------------------------------------
| SensorID |       start         |               end   | MaxReading    |
| SEN001   | 2021-01-05 08:45:00 | 2021-01-05 08:50:00 |   38.5        |
| SEN001   | 2021-01-05 08:50:00 | 2021-01-05 09:13:00 |   39.6        |


-------------------------------------------------------------------------







 
