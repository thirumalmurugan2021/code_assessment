Problem Statement:
A sensor emits a reading at a regular interval and that goes to a Kafka topic.  The reading 
The key is the Sensor ID, created time is the event time and reading is the value.  This is a patient monitoring system where the sensor is attached to a patient measuring the value.
The first two columns are start and end time, third column is the max reading value, received in the most recent 15 minutes
The solution is to get the maximum temperature in the 15-minute window 
If we say, the max temp is 38.5 at 9 am, which means we received some temperature reading from 8:45 to 9:00 and the max observation was 38.5
However, we are asked to report the outcome every five minutes
The sensor reading looks as below
SEN001:{"CreatedTime": "2021-01-05 08:45:00","Reading": 36.2}
SEN001:{"CreatedTime": "2021-01-05 08:50:00","Reading": 38.5}
SEN001:{"CreatedTime": "2021-01-05 09:03:00","Reading": 37.6}
SEN001:{"CreatedTime": "2021-01-05 09:13:00","Reading": 35.5}

Solution:
If we consider, Tumbling window of 15 minutes and trigger time of 5 minutes
The windows will not overlap. But in our case, it overlaps. Each window overlaps by 10 minutes. So, we need to use Sliding Window.
Sliding windows are an excellent tool for computing moving aggregates
Each event is most likely to participate in more than one window due to the overlaps.

We construct a schema for the sensor data
val sensorSchema = StructType(List(
      StructField("CreatedTime", StringType),
      StructField("Reading", DoubleType)
    ))
Create a Dataframe to read the data from the kafka topic as below
val kafkaSrcDF = spark.
		    readStream.
                    format("kafka").
                    option("kafka.bootstrap.servers", "localhost:9092").
                    option("subscribe", "sensor").
                    option("startingOffsets", "earliest").
                    load()

Extract the key & value data from the kafkaSrcDF and provided alias names 
val valueDF = kafkaSrcDF.
                        select(col("key").cast("string").alias("SensorID"),from_json(
			col("value").cast("string"), invoiceSchema).alias("value"))

Add a new column with time formatted as below
val sensorDF = valueDF.select("SensorID", "value.*").
		       withColumn("CreatedTime", to_timestamp(col("CreatedTime"), "yyyy-MM-dd HH:mm:ss"))

WaterMark time as 30 minutes, any data beyond 30 minutes boundry will be ignored.
Grouping by SensorID and window column as CreatedTime with 15 minute time and 5 minute as trigger time
Getting the max Reading value with every 15 minutes sliding window

val aggDF = sensorDF.withWatermark("CreatedTime", "30 minute").groupBy(col("SensorID"),
  			window(col("CreatedTime"), "15 minute", "5 minute")).
			agg(max("Reading").alias("MaxReading"))

Create output dataframe 
val outputDF = aggDF.select("SensorID", "window.start", "window.end", "MaxReading")

Writing the output stream on console 
val windowQuery = outputDF.
		  writeStream.
		  format("console").
                  outputMode("update").
                  option("checkpointLocation", "chk-point-dir").
                  trigger(Trigger.ProcessingTime("1 minute")).start()

windowQuery.awaitTermination()










 
